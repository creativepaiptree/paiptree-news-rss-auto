#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import json
import os
import time
import hashlib
import requests
from datetime import datetime, timedelta
import sys
from web_scraper import ArticleImageScraper

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
def get_config():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì •ë³´ ì½ê¸°"""
    try:
        # Google Sheets ì¸ì¦ ì •ë³´
        google_creds = os.environ.get('GOOGLE_CREDENTIALS')
        if not google_creds:
            raise ValueError("GOOGLE_CREDENTIALS í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # JSON íŒŒì‹± (í™˜ê²½ë³€ìˆ˜ëŠ” ì´ë¯¸ JSON ë¬¸ìì—´)
        try:
            creds_dict = json.loads(google_creds)
            print("âœ… Google Credentials JSON íŒŒì‹± ì„±ê³µ")
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ğŸ“ ë°›ì€ ë°ì´í„° ê¸¸ì´: {len(google_creds)} ë¬¸ì")
            print(f"ğŸ“ ë°ì´í„° ì‹œì‘ ë¶€ë¶„: {google_creds[:100]}...")
            raise ValueError(f"GOOGLE_CREDENTIALS JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # Google Sheets ID
        sheets_id = os.environ.get('GOOGLE_SHEETS_ID')
        if not sheets_id:
            raise ValueError("GOOGLE_SHEETS_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"âœ… Google Sheets ID: {sheets_id[:20]}...")
        return creds_dict, sheets_id
        
    except Exception as e:
        print(f"âŒ ì„¤ì • ì½ê¸° ì‹¤íŒ¨: {e}")
        sys.exit(1)

# RSS í”¼ë“œ ëª©ë¡ - ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (í‚¤ì›Œë“œë³„)
RSS_FEEDS = [
    "http://newssearch.naver.com/search.naver?where=rss&query=íŒŒì´í”„íŠ¸ë¦¬",
    "http://newssearch.naver.com/search.naver?where=rss&query=íŒŒë¨¸ìŠ¤ë§ˆì¸ë“œ", 
    "http://newssearch.naver.com/search.naver?where=rss&query=paiptree",
    "http://newssearch.naver.com/search.naver?where=rss&query=farmersmind"
]

# ê²€ìƒ‰ í‚¤ì›Œë“œ (ì´ë¯¸ RSSì—ì„œ í•„í„°ë§ë˜ë¯€ë¡œ ì „ì²´ ë§¤ì¹­)
KEYWORDS = [
    "íŒŒì´í”„íŠ¸ë¦¬", "íŒŒë¨¸ìŠ¤ë§ˆì¸ë“œ", "paiptree", "farmersmind"
]

def setup_google_sheets(creds_dict, sheets_id):
    """Google Sheets ì—°ê²° ì„¤ì •"""
    try:
        print("ğŸ”— Google Sheets ì—°ê²° ì¤‘...")
        
        # ì¸ì¦ ë²”ìœ„ ì„¤ì •
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        
        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦
        credentials = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        gc = gspread.authorize(credentials)
        
        # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸°
        sheet = gc.open_by_key(sheets_id)
        
        # news_data ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        try:
            worksheet = sheet.worksheet('news_data')
            print("âœ… ê¸°ì¡´ news_data ì‹œíŠ¸ ë°œê²¬")
        except gspread.WorksheetNotFound:
            print("ğŸ“ news_data ì‹œíŠ¸ ìƒì„± ì¤‘...")
            worksheet = sheet.add_worksheet(title='news_data', rows=1000, cols=21)
            
            # Materials í‘œì¤€ 21ê°œ ì»¬ëŸ¼ (A-U) í—¤ë” ì¶”ê°€
            headers = [
                'id', 'title', 'description', 'category', 'tags',                    # A-E
                'upload_date', 'file_size', 'file_format', 'dimensions',             # F-I
                'creator', 'brand_alignment', 'usage_rights', 'version',             # J-M
                'download_count', 'rating', 'thumbnail_url', 'file_url',            # N-Q
                'original_url', 'status', 'featured', 'created_at'                  # R-U
            ]
            worksheet.append_row(headers)
            print("âœ… news_data ì‹œíŠ¸ ìƒì„± ì™„ë£Œ")
        
        return worksheet
    except Exception as e:
        print(f"âŒ Google Sheets ì—°ê²° ì‹¤íŒ¨: {e}")
        sys.exit(1)

def fetch_rss_news(rss_url, keywords, image_scraper, initial_mode=False):
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    news_items = []
    
    try:
        print(f"ğŸ“¡ RSS í”¼ë“œ í™•ì¸ ì¤‘: {rss_url}")
        
        # RSS í”¼ë“œ íŒŒì‹±
        feed = feedparser.parse(rss_url)
        
        if feed.bozo:
            print(f"âš ï¸ RSS í”¼ë“œ íŒŒì‹± ê²½ê³ : {rss_url}")
        
        total_entries = len(feed.entries) if hasattr(feed, 'entries') else 0
        print(f"ğŸ“Š ì´ {total_entries}ê°œ RSS ì—”íŠ¸ë¦¬ ë°œê²¬")
        
        # ì´ˆê¸° ëª¨ë“œì¼ ë•ŒëŠ” ëª¨ë“  ë‰´ìŠ¤ ìˆ˜ì§‘, ì¼ë°˜ ëª¨ë“œì¼ ë•ŒëŠ” ìµœê·¼ ë‰´ìŠ¤ë§Œ
        entries_to_process = feed.entries if hasattr(feed, 'entries') else []
        
        if not initial_mode:
            # ì¼ë°˜ ëª¨ë“œ: ìµœê·¼ 7ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ
            seven_days_ago = datetime.now() - timedelta(days=7)
            recent_entries = []
            for entry in entries_to_process:
                published = entry.get('published_parsed')
                if published:
                    pub_date = datetime(*published[:6])
                    if pub_date >= seven_days_ago:
                        recent_entries.append(entry)
                else:
                    # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ìµœê·¼ìœ¼ë¡œ ê°„ì£¼
                    recent_entries.append(entry)
            entries_to_process = recent_entries
            print(f"ğŸ—“ï¸ ìµœê·¼ 7ì¼ ì´ë‚´ ë‰´ìŠ¤: {len(entries_to_process)}ê°œ")
        else:
            print(f"ğŸ¯ ì´ˆê¸° ìˆ˜ì§‘ ëª¨ë“œ: ëª¨ë“  ê°€ëŠ¥í•œ ë‰´ìŠ¤ ({len(entries_to_process)}ê°œ) ì²˜ë¦¬")
        
        # ê° ë‰´ìŠ¤ ì•„ì´í…œ í™•ì¸
        for entry in entries_to_process:
            title = entry.get('title', '')
            description = entry.get('description', '') or entry.get('summary', '')
            link = entry.get('link', '')
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
            content_to_check = f"{title} {description}".lower()
            matched_keywords = [kw for kw in keywords if kw.lower() in content_to_check]
            
            if matched_keywords:
                # ë°œí–‰ì¼ì ì²˜ë¦¬
                published = entry.get('published_parsed')
                if published:
                    pub_date = datetime(*published[:6]).strftime('%Y-%m-%d')
                    pub_datetime = datetime(*published[:6])
                else:
                    pub_date = datetime.now().strftime('%Y-%m-%d')
                    pub_datetime = datetime.now()
                
                # ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
                source = feed.feed.get('title', 'Unknown')
                
                # ğŸ”¥ ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„ (RSS â†’ ì›ë³¸ ê¸°ì‚¬ â†’ ê¸°ë³¸ ì´ë¯¸ì§€)
                thumbnail_url = ""
                
                # 1. RSSì—ì„œ ì œê³µí•˜ëŠ” ì´ë¯¸ì§€ ìš°ì„  ì‹œë„
                if hasattr(entry, 'media_thumbnail'):
                    thumbnail_url = entry.media_thumbnail[0]['url'] if entry.media_thumbnail else ""
                elif hasattr(entry, 'enclosures') and entry.enclosures:
                    for enc in entry.enclosures:
                        if enc.type.startswith('image/'):
                            thumbnail_url = enc.href
                            break
                
                # 2. RSS ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œ ğŸ”¥
                if not thumbnail_url and link:
                    try:
                        print(f"ğŸ–¼ï¸ ì›ë³¸ ê¸°ì‚¬ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„: {link}")
                        # ğŸ”¥ ìµœì í™”ëœ ì´ë¯¸ì§€ ì¶”ì¶œ (400x300 ìµœëŒ€ í¬ê¸°)
                        extracted_image = image_scraper.extract_largest_image(
                            link, 
                            max_size=(400, 300),  # ì¸ë„¤ì¼ìš© ìµœì  í¬ê¸°
                            return_optimized=True  # ì••ì¶• ë° ë¦¬ì‚¬ì´ì§• í™œì„±í™”
                        )
                        if extracted_image:
                            thumbnail_url = extracted_image
                            print(f"âœ… ìµœì í™”ëœ ì´ë¯¸ì§€ ì¶”ì¶œ ì„±ê³µ!")
                            if extracted_image.startswith('data:image/'):
                                print(f"ğŸ“ Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ({len(extracted_image)} ë¬¸ì)")
                            else:
                                print(f"ğŸ”— ì›ë³¸ URL: {extracted_image}")
                        else:
                            print(f"âŒ ì›ë³¸ ê¸°ì‚¬ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {link}")
                    except Exception as e:
                        print(f"âš ï¸ ì´ë¯¸ì§€ ì¶”ì¶œ ì˜ˆì™¸: {e}")
                
                # 3. ì—¬ì „íˆ ì—†ìœ¼ë©´ í‚¤ì›Œë“œë³„ ê¸°ë³¸ ì´ë¯¸ì§€
                if not thumbnail_url:
                    thumbnail_url = get_default_image_by_keywords(matched_keywords)
                
                news_item = {
                    'title': title[:200],  # ì œëª© ê¸¸ì´ ì œí•œ
                    'description': description[:500],  # ì„¤ëª… ê¸¸ì´ ì œí•œ
                    'category': source,
                    'tags': ','.join(matched_keywords),
                    'upload_date': pub_date,
                    'pub_datetime': pub_datetime,  # ì •ë ¬ìš©
                    'download_count': 0,  # ê¸°ë³¸ê°’ 0
                    'thumbnail_url': thumbnail_url,
                    'original_url': link
                }
                
                news_items.append(news_item)
                print(f"âœ… í‚¤ì›Œë“œ ë§¤ì¹­: {title[:50]}... (í‚¤ì›Œë“œ: {matched_keywords}) [{pub_date}]")
        
        print(f"ğŸ“Š {rss_url}ì—ì„œ {len(news_items)}ê°œ ë§¤ì¹­ ë‰´ìŠ¤ ë°œê²¬")
        return news_items
        
    except Exception as e:
        print(f"âŒ RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨ {rss_url}: {e}")
        return []

def generate_sequential_id(worksheet):
    """ê¸°ì¡´ ë°ì´í„° í™•ì¸í•´ì„œ ë‹¤ìŒ ë²ˆí˜¸ ìƒì„± (001, 002, 003...)"""
    try:
        all_records = worksheet.get_all_records()
        if not all_records:
            return "001"
        
        # ê¸°ì¡´ IDì—ì„œ ìˆ«ì ì¶”ì¶œí•´ì„œ ìµœëŒ€ê°’ ì°¾ê¸°
        max_num = 0
        for record in all_records:
            try:
                current_id = str(record.get('id', '0'))
                # ìˆ«ìë§Œ ì¶”ì¶œ (ì•ì˜ 0 ì œê±°)
                num = int(current_id.lstrip('0')) if current_id.strip() else 0
                max_num = max(max_num, num)
            except (ValueError, TypeError):
                continue
        
        # ë‹¤ìŒ ë²ˆí˜¸ë¥¼ 3ìë¦¬ë¡œ í¬ë§·íŒ…
        next_num = max_num + 1
        return f"{next_num:03d}"
        
    except Exception as e:
        print(f"âš ï¸ ID ìƒì„± ì‹¤íŒ¨, ì„ì‹œ ID ì‚¬ìš©: {e}")
        # ì‹¤íŒ¨ì‹œ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ID
        return f"{int(time.time() % 10000):04d}"

def is_duplicate_news(worksheet, original_url):
    """ì¤‘ë³µ ë‰´ìŠ¤ í™•ì¸ (URL ê¸°ë°˜)"""
    try:
        # ëª¨ë“  ê¸°ì¡´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        all_records = worksheet.get_all_records()
        
        for record in all_records:
            # URLì´ ì¼ì¹˜í•˜ë©´ ì¤‘ë³µ
            if record.get('original_url') == original_url:
                return True
        
        return False
    except Exception as e:
        print(f"âš ï¸ ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

def get_default_image_by_keywords(matched_keywords):
    """í‚¤ì›Œë“œë³„ ê¸°ë³¸ ì´ë¯¸ì§€ URL ë°˜í™˜"""
    # í‚¤ì›Œë“œë³„ ê³ í’ˆì§ˆ ê¸°ë³¸ ì´ë¯¸ì§€ ë§¤í•‘
    default_images = {
        'paiptree': 'https://example.com/paiptree-logo.jpg',
        'farmersmind': 'https://example.com/farmersmind-logo.jpg',
        'íŒŒì´í”„íŠ¸ë¦¬': 'https://example.com/paiptree-kr.jpg',
        'íŒŒë¨¸ìŠ¤ë§ˆì¸ë“œ': 'https://example.com/farmersmind-kr.jpg'
    }
    
    # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ì¤‘ ì²« ë²ˆì§¸ë¡œ ê¸°ë³¸ ì´ë¯¸ì§€ ì„ íƒ
    for keyword in matched_keywords:
        if keyword.lower() in default_images:
            return default_images[keyword.lower()]
    
    # ë§¤ì¹­ë˜ëŠ” ê¸°ë³¸ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
    return ""

def add_news_to_sheet(worksheet, news_item):
    """Google Sheetsì— ë‰´ìŠ¤ ì¶”ê°€"""
    try:
        # ì¤‘ë³µ í™•ì¸ (URL ê¸°ë°˜)
        if is_duplicate_news(worksheet, news_item['original_url']):
            print(f"â­ï¸ ì¤‘ë³µ ë‰´ìŠ¤ ìŠ¤í‚µ: {news_item['title'][:50]}...")
            return False
        
        # ìˆœì°¨ ID ìƒì„±
        news_id = generate_sequential_id(worksheet)
        
        # Materials í‘œì¤€ 21ê°œ ì»¬ëŸ¼ì— ë§ì¶˜ ë°ì´í„° ìƒì„±
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        row_data = [
            news_id,                                    # A: id
            news_item['title'],                         # B: title
            news_item['description'],                   # C: description
            news_item['category'],                      # D: category (ì–¸ë¡ ì‚¬)
            news_item['tags'],                          # E: tags
            news_item['upload_date'],                   # F: upload_date
            'N/A',                                      # G: file_size (ë‰´ìŠ¤ëŠ” íŒŒì¼ ì•„ë‹˜)
            'news',                                     # H: file_format (ë‰´ìŠ¤ íƒ€ì…)
            'N/A',                                      # I: dimensions
            news_item['category'],                      # J: creator (ì–¸ë¡ ì‚¬ëª…)
            'high',                                     # K: brand_alignment (ë¸Œëœë“œ ì—°ê´€ë„ ë†’ìŒ)
            'read-only',                               # L: usage_rights (ì½ê¸°ì „ìš©)
            '1.0',                                      # M: version
            news_item['download_count'],                # N: download_count
            '0',                                        # O: rating (ê¸°ë³¸ 0ì )
            news_item['thumbnail_url'],                 # P: thumbnail_url
            news_item['original_url'],                  # Q: file_url (ì›ë¬¸ ë§í¬)
            news_item['original_url'],                  # R: original_url (ë™ì¼)
            'active',                                   # S: status (í™œì„± ìƒíƒœ)
            'false',                                    # T: featured (ê¸°ë³¸ ë¹„ì¶”ì²œ)
            current_time                                # U: created_at (ìƒì„±ì‹œê°„)
        ]
        
        worksheet.append_row(row_data)
        print(f"âœ… ë‰´ìŠ¤ ì¶”ê°€ (ID: {news_id}): {news_item['title'][:50]}...")
        return True
        
    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Paiptree ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ì´ˆê¸° ìˆ˜ì§‘ ëª¨ë“œ í™•ì¸ (í™˜ê²½ë³€ìˆ˜)
    initial_mode = os.environ.get('INITIAL_COLLECTION', 'false').lower() == 'true'
    
    if initial_mode:
        print("ğŸ¯ ì´ˆê¸° ëŒ€ëŸ‰ ìˆ˜ì§‘ ëª¨ë“œ í™œì„±í™”")
        print("ğŸ“š ê°€ëŠ¥í•œ ëª¨ë“  ê³¼ê±° ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    else:
        print("ğŸ“… ì¼ë°˜ ëª¨ë“œ: ìµœê·¼ 7ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘")
    
    start_time = time.time()
    
    # ì„¤ì • ì½ê¸°
    creds_dict, sheets_id = get_config()
    
    # Google Sheets ì—°ê²°
    worksheet = setup_google_sheets(creds_dict, sheets_id)
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘
    total_collected = 0
    total_found = 0
    all_news_items = []
    
    print(f"ğŸ” {len(RSS_FEEDS)}ê°œ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
    print(f"ğŸ·ï¸ ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(KEYWORDS)}")
    
    # ğŸ”¥ ì´ë¯¸ì§€ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” ì¤‘...")
    image_scraper = ArticleImageScraper(timeout=10, max_retries=2)
    
    for rss_url in RSS_FEEDS:
        news_items = fetch_rss_news(rss_url, KEYWORDS, image_scraper, initial_mode)
        total_found += len(news_items)
        all_news_items.extend(news_items)
    
    # ë°œí–‰ì¼ì ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
    all_news_items.sort(key=lambda x: x.get('pub_datetime', datetime.now()))
    
    print(f"\nğŸ“Š ì´ {len(all_news_items)}ê°œ ë‰´ìŠ¤ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¶”ê°€ ì¤‘...")
    
    # ì‹œíŠ¸ì— ì¶”ê°€
    for news_item in all_news_items:
        if add_news_to_sheet(worksheet, news_item):
            total_collected += 1
        
        # API í˜¸ì¶œ ì œí•œ ê³ ë ¤í•˜ì—¬ ì ì‹œ ëŒ€ê¸°
        time.sleep(0.5)
    
    # ì‹¤í–‰ ê²°ê³¼
    end_time = time.time()
    execution_time = round(end_time - start_time, 2)
    
    print(f"\nğŸ‰ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ğŸ¯ ìˆ˜ì§‘ ëª¨ë“œ: {'ì´ˆê¸° ëŒ€ëŸ‰ ìˆ˜ì§‘' if initial_mode else 'ì¼ë°˜ ìˆ˜ì§‘'}")
    print(f"ğŸ“Š ì´ ë°œê²¬: {total_found}ê°œ")
    print(f"âœ… ìƒˆë¡œ ì¶”ê°€: {total_collected}ê°œ")
    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time}ì´ˆ")
    
    if total_collected == 0:
        print("â„¹ï¸ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"ğŸŒŸ {total_collected}ê°œì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
