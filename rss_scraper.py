#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import json
import os
import time
import hashlib
import requests
from datetime import datetime, timedelta
import sys
import re
from urllib.parse import urlparse
from html import unescape
import dateutil.parser
from difflib import SequenceMatcher

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
def get_config():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì •ë³´ ì½ê¸°"""
    try:
        # Google Sheets ì¸ì¦ ì •ë³´
        google_creds = os.environ.get('GOOGLE_CREDENTIALS')
        if not google_creds:
            raise ValueError("GOOGLE_CREDENTIALS í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # JSON íŒŒì‹± (í™˜ê²½ë³€ìˆ˜ëŠ” ì´ë¯¸ JSON ë¬¸ìì—´)
        try:
            creds_dict = json.loads(google_creds)
            print("âœ… Google Credentials JSON íŒŒì‹± ì„±ê³µ")
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ğŸ“ ë°›ì€ ë°ì´í„° ê¸¸ì´: {len(google_creds)} ë¬¸ì")
            print(f"ğŸ“ ë°ì´í„° ì‹œì‘ ë¶€ë¶„: {google_creds[:100]}...")
            raise ValueError(f"GOOGLE_CREDENTIALS JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # Google Sheets ID
        sheets_id = os.environ.get('GOOGLE_SHEETS_ID')
        if not sheets_id:
            raise ValueError("GOOGLE_SHEETS_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"âœ… Google Sheets ID: {sheets_id[:20]}...")
        return creds_dict, sheets_id
        
    except Exception as e:
        print(f"âŒ ì„¤ì • ì½ê¸° ì‹¤íŒ¨: {e}")
        sys.exit(1)

# RSS í”¼ë“œ ëª©ë¡ - Google News ê²€ìƒ‰ (í‚¤ì›Œë“œë³„)
RSS_FEEDS = [
    "https://news.google.com/rss/search?q=íŒŒì´í”„íŠ¸ë¦¬&hl=ko&gl=KR&ceid=KR:ko",
    "https://news.google.com/rss/search?q=íŒŒë¨¸ìŠ¤ë§ˆì¸ë“œ&hl=ko&gl=KR&ceid=KR:ko",
    "https://news.google.com/rss/search?q=paiptree&hl=ko&gl=KR&ceid=KR:ko",
    "https://news.google.com/rss/search?q=farmersmind&hl=ko&gl=KR&ceid=KR:ko"
]

# ê²€ìƒ‰ í‚¤ì›Œë“œ (ì´ë¯¸ RSSì—ì„œ í•„í„°ë§ë˜ë¯€ë¡œ ì „ì²´ ë§¤ì¹­)
KEYWORDS = [
    "íŒŒì´í”„íŠ¸ë¦¬", "íŒŒë¨¸ìŠ¤ë§ˆì¸ë“œ", "paiptree", "farmersmind"
]

def setup_google_sheets(creds_dict, sheets_id):
    """Google Sheets ì—°ê²° ì„¤ì •"""
    try:
        print("ğŸ”— Google Sheets ì—°ê²° ì¤‘...")
        
        # ì¸ì¦ ë²”ìœ„ ì„¤ì •
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        
        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦
        credentials = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        gc = gspread.authorize(credentials)
        
        # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸°
        sheet = gc.open_by_key(sheets_id)
        
        # news_data ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        try:
            worksheet = sheet.worksheet('news_data')
            print("âœ… ê¸°ì¡´ news_data ì‹œíŠ¸ ë°œê²¬")
        except gspread.WorksheetNotFound:
            print("ğŸ“ news_data ì‹œíŠ¸ ìƒì„± ì¤‘...")
            worksheet = sheet.add_worksheet(title='news_data', rows=1000, cols=9)
            
            # 9ê°œ ì»¬ëŸ¼ í—¤ë” ì¶”ê°€ (ê¸°ì¡´ ì‹œíŠ¸ êµ¬ì¡°ì— ë§ì¶¤)
            headers = [
                'id', 'title', 'description', 'category', 'tags',
                'upload_date', 'download_count', 'thumbnail_url', 'original_url'
            ]
            worksheet.append_row(headers)
            print("âœ… news_data ì‹œíŠ¸ ìƒì„± ì™„ë£Œ")
        
        return worksheet
    except Exception as e:
        print(f"âŒ Google Sheets ì—°ê²° ì‹¤íŒ¨: {e}")
        sys.exit(1)

def extract_domain_name(url):
    """URLì—ì„œ ë„ë©”ì¸ëª… ì¶”ì¶œ"""
    try:
        domain = urlparse(url).netloc
        # www. ì œê±° ë° ì²« ë²ˆì§¸ ë„ë©”ì¸ë§Œ ì¶”ì¶œ
        domain = re.sub(r'^www\.', '', domain)
        return domain.split('.')[0] if domain else "Unknown"
    except:
        return "Unknown"

def extract_source_from_title(title):
    """ì œëª© ëì—ì„œ ì¶œì²˜ ì¶”ì¶œ (- ë’¤ì˜ ë‚´ìš©)"""
    if ' - ' in title:
        parts = title.rsplit(' - ', 1)  # ë§ˆì§€ë§‰ '-' ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
        if len(parts) == 2:
            clean_title = parts[0].strip()
            source = parts[1].strip()
            return clean_title, source
    
    return title, "Unknown"

def clean_source_name(source):
    """ì¶œì²˜ëª… ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°)"""
    if not source or source == "Unknown":
        return source
    
    # ìì£¼ ë‚˜ì˜¤ëŠ” ë¶ˆí•„ìš”í•œ ë‹¨ì–´ë“¤ ì œê±°
    cleaners = [
        r'\s*ë‰´ìŠ¤$', r'\s*ì‹ ë¬¸$', r'\s*ì¼ë³´$', 
        r'^\s*', r'\s*$',  # ì•ë’¤ ê³µë°±
        r'\.{2,}$'  # ëì˜ ì ë“¤ ì œê±°
    ]
    
    clean_source = source
    for cleaner in cleaners:
        clean_source = re.sub(cleaner, '', clean_source)
    
    # ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
    if len(clean_source.strip()) < 2:
        return source
    
    return clean_source.strip()

def extract_publish_date(entry):
    """RSSì—ì„œ ì •í™•í•œ ë°œí–‰ì¼ ì¶”ì¶œ"""
    # ìš°ì„ ìˆœìœ„: published_parsed > published > updated_parsed > updated
    for date_field in ['published_parsed', 'updated_parsed']:
        if hasattr(entry, date_field) and entry.get(date_field):
            try:
                return datetime(*entry[date_field][:6])
            except:
                continue
    
    # ë¬¸ìì—´ ë‚ ì§œ íŒŒì‹± ì‹œë„
    for date_field in ['published', 'updated']:
        if entry.get(date_field):
            try:
                return dateutil.parser.parse(entry[date_field])
            except:
                continue
    
    print("âš ï¸ ë‚ ì§œ ì •ë³´ ì—†ìŒ, í˜„ì¬ ì‹œê°„ ì‚¬ìš©")
    return datetime.now()

def clean_description(raw_description):
    """HTML íƒœê·¸ ì œê±°í•˜ê³  ê¹”ë”í•œ í•œê¸€ ë‚´ìš©ë§Œ ì¶”ì¶œ"""
    if not raw_description:
        return "ë‰´ìŠ¤ ë‚´ìš©ì€ ì›ë¬¸ ë§í¬ì—ì„œ í™•ì¸í•˜ì„¸ìš”"
    
    # HTML íƒœê·¸ ì œê±°
    clean_text = re.sub(r'<[^>]+>', '', raw_description)
    
    # HTML ì—”í‹°í‹° ë””ì½”ë”© (&lt; &gt; &amp; ë“±)
    clean_text = unescape(clean_text)
    
    # ì—°ì†ëœ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ìœ ì§€)
    clean_text = re.sub(r'[^\w\s\.\,\!\?\:\;\(\)\-\'\"â€¦]', '', clean_text, flags=re.UNICODE)
    
    # ë¹ˆ ë‚´ìš© ì²´í¬
    if not clean_text or len(clean_text.strip()) < 10:
        return "ë‰´ìŠ¤ ë‚´ìš©ì€ ì›ë¬¸ ë§í¬ì—ì„œ í™•ì¸í•˜ì„¸ìš”"
    
    return clean_text[:500]

def categorize_news(title, description, source):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜"""
    content = f"{title} {description}".lower()
    
    # ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë§¤í•‘
    categories = {
        'ê²½ì œ/íˆ¬ì': ['íˆ¬ì', 'í€ë”©', 'íˆ¬ìê¸ˆ', 'ì‹œë¦¬ì¦ˆ', 'ê¸°ì—…ê³µê°œ', 'ìƒì¥', 'ë§¤ì¶œ', 'ìˆ˜ìµ', 'ìê¸ˆì¡°ë‹¬', 'ê·¸ë¦°ë©ìŠ¤'],
        'ê¸°ìˆ /AI': ['ai', 'ì¸ê³µì§€ëŠ¥', 'ìŠ¤ë§ˆíŠ¸íŒœ', 'í”Œë«í¼', 'ê¸°ìˆ ', 'ì†”ë£¨ì…˜', 'ì‹œìŠ¤í…œ', 'íŒŒë¨¸ìŠ¤ë§ˆì¸ë“œ', 'farmersmind'],
        'ì‚¬ì—…í™•ì¥': ['ì§„ì¶œ', 'í˜‘ë ¥', 'ì—…ë¬´í˜‘ì•½', 'íŒŒíŠ¸ë„ˆì‹­', 'í™•ì¥', 'ê¸€ë¡œë²Œ', 'cpf', 'í† ìì´', 'í•œí˜¸ìš´ìˆ˜'],
        'ë†ì¶•ì‚°ì—…': ['ì–‘ê³„', 'ì¶•ì‚°', 'ë†ê°€', 'ì¡°ë¥˜ë…ê°', 'ì§ˆë³‘ì˜ˆì°°', 'ìƒê³„ë¬¼ë¥˜', 'ìœ¡ê³„ì‹œì¥', 'ê´€ì œì‹œìŠ¤í…œ'],
        'ì–¸ë¡ /ì¸í„°ë·°': ['ì¸í„°ë·°', 'ëŒ€í‘œ', 'ì°½ì—…ì', 'ceo', 'ìŠ¤íƒ€íŠ¸ì—…', 'ê³µë™ëŒ€í‘œ'],
        'í–‰ì‚¬/ì „ì‹œ': ['afro', 'ì „ì‹œ', 'ì»¨í¼ëŸ°ìŠ¤', 'ì„¸ë¯¸ë‚˜', 'ë°œí‘œ', 'ì†Œê°œ'],
        'ì—°êµ¬ê°œë°œ': ['ê±´êµ­ëŒ€', 'ì—°êµ¬', 'ê°œë°œ', 'ê³µë™ì—°êµ¬', 'ëŒ€í•™', 'í•™ìˆ '],
    }
    
    for category, keywords in categories.items():
        if any(keyword in content for keyword in keywords):
            return category
    
    return 'ê¸°íƒ€ë‰´ìŠ¤'

def calculate_similarity(title1, title2):
    """ë‘ ì œëª©ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1)"""
    # ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ë¹„êµ
    clean_title1 = re.sub(r'[^\w]', '', title1, flags=re.UNICODE)
    clean_title2 = re.sub(r'[^\w]', '', title2, flags=re.UNICODE)
    
    return SequenceMatcher(None, clean_title1, clean_title2).ratio()

def deduplicate_news_comprehensive(all_news_items):
    """ì œëª© ìœ ì‚¬ë„ + URL ê¸°ë°˜ ì¢…í•© ì¤‘ë³µ ì œê±°"""
    unique_news = []
    seen_titles = []
    seen_urls = set()
    
    print(f"ğŸ” ì¤‘ë³µ ì œê±° ì‹œì‘: {len(all_news_items)}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘...")
    
    for news in all_news_items:
        title = news['title'].lower().strip()
        clean_url = news['original_url'].split('?')[0]  # URL íŒŒë¼ë¯¸í„° ì œê±°
        
        # URL ì¤‘ë³µ ì²´í¬
        if clean_url in seen_urls:
            print(f"ğŸ”„ ì¤‘ë³µ URL ì œê±°: {title[:50]}...")
            continue
        
        # ì œëª© ìœ ì‚¬ë„ ì²´í¬ (85% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨)
        is_similar = False
        for seen_title in seen_titles:
            similarity = calculate_similarity(title, seen_title)
            if similarity > 0.85:
                print(f"ğŸ”„ ìœ ì‚¬ ì œëª© ì œê±°: {title[:50]}... (ìœ ì‚¬ë„: {similarity:.2f})")
                is_similar = True
                break
        
        if not is_similar:
            seen_titles.append(title)
            seen_urls.add(clean_url)
            unique_news.append(news)
            print(f"âœ… ê³ ìœ  ë‰´ìŠ¤ ì¶”ê°€: {title[:50]}...")
    
    print(f"ğŸ“Š ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(unique_news)}ê°œ ë‚¨ìŒ ({len(all_news_items) - len(unique_news)}ê°œ ì œê±°)")
    return unique_news

def fetch_rss_news(rss_url, keywords, initial_mode=False):
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    news_items = []
    
    try:
        print(f"ğŸ“¡ RSS í”¼ë“œ í™•ì¸ ì¤‘: {rss_url}")
        
        # RSS í”¼ë“œ íŒŒì‹±
        feed = feedparser.parse(rss_url)
        
        if feed.bozo:
            print(f"âš ï¸ RSS í”¼ë“œ íŒŒì‹± ê²½ê³ : {rss_url}")
        
        total_entries = len(feed.entries) if hasattr(feed, 'entries') else 0
        print(f"ğŸ“Š ì´ {total_entries}ê°œ RSS ì—”íŠ¸ë¦¬ ë°œê²¬")
        
        # ì´ˆê¸° ëª¨ë“œì¼ ë•ŒëŠ” ëª¨ë“  ë‰´ìŠ¤ ìˆ˜ì§‘, ì¼ë°˜ ëª¨ë“œì¼ ë•ŒëŠ” ìµœê·¼ ë‰´ìŠ¤ë§Œ
        entries_to_process = feed.entries if hasattr(feed, 'entries') else []
        
        if not initial_mode:
            # ì¼ë°˜ ëª¨ë“œ: ìµœê·¼ 7ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ
            seven_days_ago = datetime.now() - timedelta(days=7)
            recent_entries = []
            for entry in entries_to_process:
                pub_datetime = extract_publish_date(entry)
                if pub_datetime >= seven_days_ago:
                    recent_entries.append(entry)
            entries_to_process = recent_entries
            print(f"ğŸ—“ï¸ ìµœê·¼ 7ì¼ ì´ë‚´ ë‰´ìŠ¤: {len(entries_to_process)}ê°œ")
        else:
            print(f"ğŸ¯ ì´ˆê¸° ìˆ˜ì§‘ ëª¨ë“œ: ëª¨ë“  ê°€ëŠ¥í•œ ë‰´ìŠ¤ ({len(entries_to_process)}ê°œ) ì²˜ë¦¬")
        
        # ê° ë‰´ìŠ¤ ì•„ì´í…œ í™•ì¸
        for entry in entries_to_process:
            original_title = entry.get('title', '')
            raw_description = entry.get('description', '') or entry.get('summary', '')
            link = entry.get('link', '')
            
            # ğŸ”¥ ì œëª©ì—ì„œ ì¶œì²˜ ì¶”ì¶œ
            clean_title, source = extract_source_from_title(original_title)
            source = clean_source_name(source)
            
            # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ (ì¶œì²˜ ì—†ì„ ê²½ìš° ëŒ€ì•ˆ)
            if source == "Unknown":
                source = extract_domain_name(link)
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸ (ê¹”ë”í•œ ì œëª©ìœ¼ë¡œ)
            content_to_check = f"{clean_title} {raw_description}".lower()
            matched_keywords = [kw for kw in keywords if kw.lower() in content_to_check]
            
            if matched_keywords:
                # ì •í™•í•œ ë°œí–‰ì¼ì ì¶”ì¶œ
                pub_datetime = extract_publish_date(entry)
                pub_date = pub_datetime.strftime('%Y-%m-%d')
                
                # HTML íƒœê·¸ ì œê±°í•œ ê¹”ë”í•œ ì„¤ëª…
                clean_desc = clean_description(raw_description)
                
                # ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„
                thumbnail_url = ""
                if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    thumbnail_url = entry.media_thumbnail[0].get('url', '')
                elif hasattr(entry, 'enclosures') and entry.enclosures:
                    for enc in entry.enclosures:
                        if hasattr(enc, 'type') and enc.type.startswith('image/'):
                            thumbnail_url = enc.href
                            break
                
                # ì¸ë„¤ì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¯¸ì§€
                if not thumbnail_url:
                    thumbnail_url = "https://via.placeholder.com/300x200/00B0EB/FFFFFF?text=Paiptree+News"
                
                news_item = {
                    'title': clean_title[:200] if clean_title else "ì œëª© ì—†ìŒ",  # ì¶œì²˜ ì œê±°ëœ ê¹”ë”í•œ ì œëª©
                    'description': clean_desc,
                    'category': source,  # ì‹¤ì œ ì–¸ë¡ ì‚¬ëª… (ë§¤ì¼ê²½ì œ, ë¨¸ë‹ˆíˆ¬ë°ì´ ë“±)
                    'tags': ','.join(matched_keywords),
                    'upload_date': pub_date,  # ì‹¤ì œ ë‰´ìŠ¤ ë°œí–‰ì¼
                    'pub_datetime': pub_datetime,  # ì •ë ¬ìš©
                    'download_count': 0,
                    'thumbnail_url': thumbnail_url,
                    'original_url': link
                }
                
                news_items.append(news_item)
                print(f"âœ… í‚¤ì›Œë“œ ë§¤ì¹­: {clean_title[:50]}... (ì¶œì²˜: {source}) [{pub_date}]")
        
        print(f"ğŸ“Š {rss_url}ì—ì„œ {len(news_items)}ê°œ ë§¤ì¹­ ë‰´ìŠ¤ ë°œê²¬")
        return news_items
        
    except Exception as e:
        print(f"âŒ RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨ {rss_url}: {e}")
        return []

def generate_simple_id():
    """ê°„ë‹¨í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ID ìƒì„± (API í˜¸ì¶œ ì—†ìŒ)"""
    timestamp = int(time.time())
    return f"{timestamp % 100000:05d}"

def add_news_to_sheet(worksheet, news_item):
    """Google Sheetsì— ë‰´ìŠ¤ ì¶”ê°€ - 9ê°œ ì»¬ëŸ¼ ë²„ì „"""
    try:
        # ê°„ë‹¨í•œ ID ìƒì„± (API í˜¸ì¶œ ì—†ìŒ)
        news_id = generate_simple_id()
        
        # 9ê°œ ì»¬ëŸ¼ì— ë§ì¶˜ ë°ì´í„° ìƒì„±
        row_data = [
            news_id,                        # A: id
            news_item['title'],             # B: title (ì¶œì²˜ ì œê±°ëœ ê¹”ë”í•œ ì œëª©)
            news_item['description'],       # C: description (HTML íƒœê·¸ ì œê±°ë¨)
            news_item['category'],          # D: category (ì‹¤ì œ ì–¸ë¡ ì‚¬ëª…)
            news_item['tags'],              # E: tags
            news_item['upload_date'],       # F: upload_date (ì‹¤ì œ ë‰´ìŠ¤ ë‚ ì§œ)
            news_item['download_count'],    # G: download_count
            news_item['thumbnail_url'],     # H: thumbnail_url
            news_item['original_url']       # I: original_url
        ]
        
        worksheet.append_row(row_data)
        print(f"âœ… ë‰´ìŠ¤ ì¶”ê°€ (ID: {news_id}): {news_item['title'][:50]}...")
        
        # API í˜¸ì¶œ ì œí•œ ê³ ë ¤í•˜ì—¬ ì—¬ìœ ë¡­ê²Œ ëŒ€ê¸° (3ì´ˆ)
        time.sleep(3.0)
        return True
        
    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨, ìŠ¤í‚µ: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Paiptree ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ì¶œì²˜ ì¶”ì¶œ ë²„ì „)")
    print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ì´ˆê¸° ìˆ˜ì§‘ ëª¨ë“œ í™•ì¸ (í™˜ê²½ë³€ìˆ˜)
    initial_mode = os.environ.get('INITIAL_COLLECTION', 'false').lower() == 'true'
    
    if initial_mode:
        print("ğŸ¯ ì´ˆê¸° ëŒ€ëŸ‰ ìˆ˜ì§‘ ëª¨ë“œ í™œì„±í™”")
        print("ğŸ“š ê°€ëŠ¥í•œ ëª¨ë“  ê³¼ê±° ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    else:
        print("ğŸ“… ì¼ë°˜ ëª¨ë“œ: ìµœê·¼ 7ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘")
    
    print("ğŸ”§ ê°œì„ ì‚¬í•­: ì •í™•í•œ ë‚ ì§œ, HTML ì œê±°, ì‹¤ì œ ì¶œì²˜ ì¹´í…Œê³ ë¦¬, ì¤‘ë³µ ì œê±°")
    
    start_time = time.time()
    
    # ì„¤ì • ì½ê¸°
    creds_dict, sheets_id = get_config()
    
    # Google Sheets ì—°ê²°
    worksheet = setup_google_sheets(creds_dict, sheets_id)
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘
    total_collected = 0
    total_found = 0
    all_news_items = []
    
    print(f"ğŸ” {len(RSS_FEEDS)}ê°œ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
    print(f"ğŸ·ï¸ ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(KEYWORDS)}")
    
    # ëª¨ë“  RSSì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
    for rss_url in RSS_FEEDS:
        news_items = fetch_rss_news(rss_url, KEYWORDS, initial_mode)
        total_found += len(news_items)
        all_news_items.extend(news_items)
    
    print(f"\nğŸ“Š ìˆ˜ì§‘ ì „ ì´ ë‰´ìŠ¤: {len(all_news_items)}ê°œ")
    
    # ğŸ”¥ ì¤‘ë³µ ì œê±° ì²˜ë¦¬
    unique_news = deduplicate_news_comprehensive(all_news_items)
    
    print(f"âœ… ì¤‘ë³µ ì œê±° í›„: {len(unique_news)}ê°œ")
    print(f"ğŸ—‘ï¸ ì œê±°ëœ ì¤‘ë³µ: {len(all_news_items) - len(unique_news)}ê°œ")
    
    # ë°œí–‰ì¼ì ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
    unique_news.sort(key=lambda x: x.get('pub_datetime', datetime.now()))
    
    print(f"\nğŸ“Š ì´ {len(unique_news)}ê°œ ê³ ìœ  ë‰´ìŠ¤ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì¶”ê°€ ì¤‘...")
    print("â° ê° ë‰´ìŠ¤ë§ˆë‹¤ 3ì´ˆì”© ëŒ€ê¸°í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
    
    # ì‹œíŠ¸ì— ì¶”ê°€
    for i, news_item in enumerate(unique_news, 1):
        print(f"ğŸ“ ì§„í–‰ë¥ : {i}/{len(unique_news)}")
        if add_news_to_sheet(worksheet, news_item):
            total_collected += 1
    
    # ì‹¤í–‰ ê²°ê³¼
    end_time = time.time()
    execution_time = round(end_time - start_time, 2)
    
    print(f"\nğŸ‰ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ğŸ¯ ìˆ˜ì§‘ ëª¨ë“œ: {'ì´ˆê¸° ëŒ€ëŸ‰ ìˆ˜ì§‘' if initial_mode else 'ì¼ë°˜ ìˆ˜ì§‘'}")
    print(f"ğŸ“Š ì´ ë°œê²¬: {total_found}ê°œ")
    print(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {len(all_news_items) - len(unique_news)}ê°œ")
    print(f"âœ… ê³ ìœ  ë‰´ìŠ¤ ì¶”ê°€: {total_collected}ê°œ")
    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time}ì´ˆ")
    print(f"ğŸ”§ ê°œì„ ëœ ê¸°ëŠ¥: ì •í™•í•œ ë‚ ì§œ, ê¹”ë”í•œ ì„¤ëª…, ì‹¤ì œ ì–¸ë¡ ì‚¬ ì¹´í…Œê³ ë¦¬, ìŠ¤ë§ˆíŠ¸ ì¤‘ë³µ ì œê±°")
    
    if total_collected == 0:
        print("â„¹ï¸ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"ğŸŒŸ {total_collected}ê°œì˜ ê³ ìœ í•œ ë‰´ìŠ¤ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ“° ì¹´í…Œê³ ë¦¬: ì‹¤ì œ ì–¸ë¡ ì‚¬ëª… (ë§¤ì¼ê²½ì œ, ë¨¸ë‹ˆíˆ¬ë°ì´ ë“±)")

if __name__ == "__main__":
    main()
