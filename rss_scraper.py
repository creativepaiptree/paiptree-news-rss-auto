#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import json
import os
import time
import hashlib
import requests
from datetime import datetime, timedelta
import sys
import re
from urllib.parse import urlparse

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
def get_config():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì •ë³´ ì½ê¸°"""
    try:
        # Google Sheets ì¸ì¦ ì •ë³´
        google_creds = os.environ.get('GOOGLE_CREDENTIALS')
        if not google_creds:
            raise ValueError("GOOGLE_CREDENTIALS í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # JSON íŒŒì‹± (í™˜ê²½ë³€ìˆ˜ëŠ” ì´ë¯¸ JSON ë¬¸ìì—´)
        try:
            creds_dict = json.loads(google_creds)
            print("âœ… Google Credentials JSON íŒŒì‹± ì„±ê³µ")
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ğŸ“ ë°›ì€ ë°ì´í„° ê¸¸ì´: {len(google_creds)} ë¬¸ì")
            print(f"ğŸ“ ë°ì´í„° ì‹œì‘ ë¶€ë¶„: {google_creds[:100]}...")
            raise ValueError(f"GOOGLE_CREDENTIALS JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # Google Sheets ID
        sheets_id = os.environ.get('GOOGLE_SHEETS_ID')
        if not sheets_id:
            raise ValueError("GOOGLE_SHEETS_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"âœ… Google Sheets ID: {sheets_id[:20]}...")
        return creds_dict, sheets_id
        
    except Exception as e:
        print(f"âŒ ì„¤ì • ì½ê¸° ì‹¤íŒ¨: {e}")
        sys.exit(1)

# RSS í”¼ë“œ ëª©ë¡ - Google News ê²€ìƒ‰ (í‚¤ì›Œë“œë³„)
RSS_FEEDS = [
    "https://news.google.com/rss/search?q=íŒŒì´í”„íŠ¸ë¦¬&hl=ko&gl=KR&ceid=KR:ko",
    "https://news.google.com/rss/search?q=íŒŒë¨¸ìŠ¤ë§ˆì¸ë“œ&hl=ko&gl=KR&ceid=KR:ko",
    "https://news.google.com/rss/search?q=paiptree&hl=ko&gl=KR&ceid=KR:ko",
    "https://news.google.com/rss/search?q=farmersmind&hl=ko&gl=KR&ceid=KR:ko"
]

# ê²€ìƒ‰ í‚¤ì›Œë“œ (ì´ë¯¸ RSSì—ì„œ í•„í„°ë§ë˜ë¯€ë¡œ ì „ì²´ ë§¤ì¹­)
KEYWORDS = [
    "íŒŒì´í”„íŠ¸ë¦¬", "íŒŒë¨¸ìŠ¤ë§ˆì¸ë“œ", "paiptree", "farmersmind"
]

def setup_google_sheets(creds_dict, sheets_id):
    """Google Sheets ì—°ê²° ì„¤ì •"""
    try:
        print("ğŸ”— Google Sheets ì—°ê²° ì¤‘...")
        
        # ì¸ì¦ ë²”ìœ„ ì„¤ì •
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        
        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦
        credentials = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        gc = gspread.authorize(credentials)
        
        # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸°
        sheet = gc.open_by_key(sheets_id)
        
        # news_data ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        try:
            worksheet = sheet.worksheet('news_data')
            print("âœ… ê¸°ì¡´ news_data ì‹œíŠ¸ ë°œê²¬")
        except gspread.WorksheetNotFound:
            print("ğŸ“ news_data ì‹œíŠ¸ ìƒì„± ì¤‘...")
            worksheet = sheet.add_worksheet(title='news_data', rows=1000, cols=9)
            
            # 9ê°œ ì»¬ëŸ¼ í—¤ë” ì¶”ê°€ (ê¸°ì¡´ ì‹œíŠ¸ êµ¬ì¡°ì— ë§ì¶¤)
            headers = [
                'id', 'title', 'description', 'category', 'tags',
                'upload_date', 'download_count', 'thumbnail_url', 'original_url'
            ]
            worksheet.append_row(headers)
            print("âœ… news_data ì‹œíŠ¸ ìƒì„± ì™„ë£Œ")
        
        return worksheet
    except Exception as e:
        print(f"âŒ Google Sheets ì—°ê²° ì‹¤íŒ¨: {e}")
        sys.exit(1)

def extract_domain_name(url):
    """URLì—ì„œ ë„ë©”ì¸ëª… ì¶”ì¶œ"""
    try:
        domain = urlparse(url).netloc
        # www. ì œê±° ë° ì²« ë²ˆì§¸ ë„ë©”ì¸ë§Œ ì¶”ì¶œ
        domain = re.sub(r'^www\.', '', domain)
        return domain.split('.')[0] if domain else "Unknown"
    except:
        return "Unknown"

def fetch_rss_news(rss_url, keywords, initial_mode=False):
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    news_items = []
    
    try:
        print(f"ğŸ“¡ RSS í”¼ë“œ í™•ì¸ ì¤‘: {rss_url}")
        
        # RSS í”¼ë“œ íŒŒì‹±
        feed = feedparser.parse(rss_url)
        
        if feed.bozo:
            print(f"âš ï¸ RSS í”¼ë“œ íŒŒì‹± ê²½ê³ : {rss_url}")
        
        total_entries = len(feed.entries) if hasattr(feed, 'entries') else 0
        print(f"ğŸ“Š ì´ {total_entries}ê°œ RSS ì—”íŠ¸ë¦¬ ë°œê²¬")
        
        # ì´ˆê¸° ëª¨ë“œì¼ ë•ŒëŠ” ëª¨ë“  ë‰´ìŠ¤ ìˆ˜ì§‘, ì¼ë°˜ ëª¨ë“œì¼ ë•ŒëŠ” ìµœê·¼ ë‰´ìŠ¤ë§Œ
        entries_to_process = feed.entries if hasattr(feed, 'entries') else []
        
        if not initial_mode:
            # ì¼ë°˜ ëª¨ë“œ: ìµœê·¼ 7ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ
            seven_days_ago = datetime.now() - timedelta(days=7)
            recent_entries = []
            for entry in entries_to_process:
                published = entry.get('published_parsed')
                if published:
                    pub_date = datetime(*published[:6])
                    if pub_date >= seven_days_ago:
                        recent_entries.append(entry)
                else:
                    # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ìµœê·¼ìœ¼ë¡œ ê°„ì£¼
                    recent_entries.append(entry)
            entries_to_process = recent_entries
            print(f"ğŸ—“ï¸ ìµœê·¼ 7ì¼ ì´ë‚´ ë‰´ìŠ¤: {len(entries_to_process)}ê°œ")
        else:
            print(f"ğŸ¯ ì´ˆê¸° ìˆ˜ì§‘ ëª¨ë“œ: ëª¨ë“  ê°€ëŠ¥í•œ ë‰´ìŠ¤ ({len(entries_to_process)}ê°œ) ì²˜ë¦¬")
        
        # ê° ë‰´ìŠ¤ ì•„ì´í…œ í™•ì¸
        for entry in entries_to_process:
            title = entry.get('title', '')
            description = entry.get('description', '') or entry.get('summary', '')
            link = entry.get('link', '')
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
            content_to_check = f"{title} {description}".lower()
            matched_keywords = [kw for kw in keywords if kw.lower() in content_to_check]
            
            if matched_keywords:
                # ë°œí–‰ì¼ì ì²˜ë¦¬
                published = entry.get('published_parsed')
                if published:
                    pub_date = datetime(*published[:6]).strftime('%Y-%m-%d')
                    pub_datetime = datetime(*published[:6])
                else:
                    pub_date = datetime.now().strftime('%Y-%m-%d')
                    pub_datetime = datetime.now()
                
                # ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)
                source = feed.feed.get('title', '')
                if not source or source == 'Unknown':
                    source = extract_domain_name(link)
                
                # ë¹ˆ ì„¤ëª… ì²˜ë¦¬
                if not description or description.strip() == '':
                    description = "ë‰´ìŠ¤ ë‚´ìš©ì€ ì›ë¬¸ ë§í¬ì—ì„œ í™•ì¸í•˜ì„¸ìš”"
                
                # ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„
                thumbnail_url = ""
                if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    thumbnail_url = entry.media_thumbnail[0].get('url', '')
                elif hasattr(entry, 'enclosures') and entry.enclosures:
                    for enc in entry.enclosures:
                        if hasattr(enc, 'type') and enc.type.startswith('image/'):
                            thumbnail_url = enc.href
                            break
                
                # ì¸ë„¤ì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¯¸ì§€
                if not thumbnail_url:
                    thumbnail_url = "https://via.placeholder.com/300x200/00B0EB/FFFFFF?text=Paiptree+News"
                
                news_item = {
                    'title': title[:200] if title else "ì œëª© ì—†ìŒ",  # ì œëª© ê¸¸ì´ ì œí•œ
                    'description': description[:500] if description else "ë‚´ìš© ì—†ìŒ",  # ì„¤ëª… ê¸¸ì´ ì œí•œ
                    'category': source[:100] if source else "Unknown",
                    'tags': ','.join(matched_keywords),
                    'upload_date': pub_date,
                    'pub_datetime': pub_datetime,  # ì •ë ¬ìš©
                    'download_count': 0,  # ê¸°ë³¸ê°’ 0
                    'thumbnail_url': thumbnail_url,
                    'original_url': link
                }
                
                news_items.append(news_item)
                print(f"âœ… í‚¤ì›Œë“œ ë§¤ì¹­: {title[:50]}... (í‚¤ì›Œë“œ: {matched_keywords}) [{pub_date}]")
        
        print(f"ğŸ“Š {rss_url}ì—ì„œ {len(news_items)}ê°œ ë§¤ì¹­ ë‰´ìŠ¤ ë°œê²¬")
        return news_items
        
    except Exception as e:
        print(f"âŒ RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨ {rss_url}: {e}")
        return []

def generate_simple_id():
    """ê°„ë‹¨í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ID ìƒì„± (API í˜¸ì¶œ ì—†ìŒ)"""
    timestamp = int(time.time())
    return f"{timestamp % 100000:05d}"

def add_news_to_sheet(worksheet, news_item):
    """Google Sheetsì— ë‰´ìŠ¤ ì¶”ê°€ - 9ê°œ ì»¬ëŸ¼ ë²„ì „"""
    try:
        # ê°„ë‹¨í•œ ID ìƒì„± (API í˜¸ì¶œ ì—†ìŒ)
        news_id = generate_simple_id()
        
        # 9ê°œ ì»¬ëŸ¼ì— ë§ì¶˜ ë°ì´í„° ìƒì„±
        row_data = [
            news_id,                        # A: id
            news_item['title'],             # B: title
            news_item['description'],       # C: description
            news_item['category'],          # D: category
            news_item['tags'],              # E: tags
            news_item['upload_date'],       # F: upload_date
            news_item['download_count'],    # G: download_count
            news_item['thumbnail_url'],     # H: thumbnail_url
            news_item['original_url']       # I: original_url
        ]
        
        worksheet.append_row(row_data)
        print(f"âœ… ë‰´ìŠ¤ ì¶”ê°€ (ID: {news_id}): {news_item['title'][:50]}...")
        
        # API í˜¸ì¶œ ì œí•œ ê³ ë ¤í•˜ì—¬ ì—¬ìœ ë¡­ê²Œ ëŒ€ê¸° (3ì´ˆ)
        time.sleep(3.0)
        return True
        
    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨, ìŠ¤í‚µ: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Paiptree ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ì´ˆê¸° ìˆ˜ì§‘ ëª¨ë“œ í™•ì¸ (í™˜ê²½ë³€ìˆ˜)
    initial_mode = os.environ.get('INITIAL_COLLECTION', 'false').lower() == 'true'
    
    if initial_mode:
        print("ğŸ¯ ì´ˆê¸° ëŒ€ëŸ‰ ìˆ˜ì§‘ ëª¨ë“œ í™œì„±í™”")
        print("ğŸ“š ê°€ëŠ¥í•œ ëª¨ë“  ê³¼ê±° ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    else:
        print("ğŸ“… ì¼ë°˜ ëª¨ë“œ: ìµœê·¼ 7ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘")
    
    start_time = time.time()
    
    # ì„¤ì • ì½ê¸°
    creds_dict, sheets_id = get_config()
    
    # Google Sheets ì—°ê²°
    worksheet = setup_google_sheets(creds_dict, sheets_id)
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘
    total_collected = 0
    total_found = 0
    all_news_items = []
    
    print(f"ğŸ” {len(RSS_FEEDS)}ê°œ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
    print(f"ğŸ·ï¸ ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(KEYWORDS)}")
    print("ğŸŒ ì•ˆì „í•œ ì†ë„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ì¤‘ë³µ ì²´í¬ ì—†ìŒ, 3ì´ˆ ëŒ€ê¸°)")
    
    for rss_url in RSS_FEEDS:
        news_items = fetch_rss_news(rss_url, KEYWORDS, initial_mode)
        total_found += len(news_items)
        all_news_items.extend(news_items)
    
    # ë°œí–‰ì¼ì ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
    all_news_items.sort(key=lambda x: x.get('pub_datetime', datetime.now()))
    
    print(f"\nğŸ“Š ì´ {len(all_news_items)}ê°œ ë‰´ìŠ¤ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì¶”ê°€ ì¤‘...")
    print("â° ê° ë‰´ìŠ¤ë§ˆë‹¤ 3ì´ˆì”© ëŒ€ê¸°í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
    
    # ì‹œíŠ¸ì— ì¶”ê°€ (ì¤‘ë³µ ì²´í¬ ì—†ìŒ)
    for i, news_item in enumerate(all_news_items, 1):
        print(f"ğŸ“ ì§„í–‰ë¥ : {i}/{len(all_news_items)}")
        if add_news_to_sheet(worksheet, news_item):
            total_collected += 1
    
    # ì‹¤í–‰ ê²°ê³¼
    end_time = time.time()
    execution_time = round(end_time - start_time, 2)
    
    print(f"\nğŸ‰ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ğŸ¯ ìˆ˜ì§‘ ëª¨ë“œ: {'ì´ˆê¸° ëŒ€ëŸ‰ ìˆ˜ì§‘' if initial_mode else 'ì¼ë°˜ ìˆ˜ì§‘'}")
    print(f"ğŸ“Š ì´ ë°œê²¬: {total_found}ê°œ")
    print(f"âœ… ì„±ê³µ ì¶”ê°€: {total_collected}ê°œ")
    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time}ì´ˆ")
    
    if total_collected == 0:
        print("â„¹ï¸ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"ğŸŒŸ {total_collected}ê°œì˜ ë‰´ìŠ¤ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ì¤‘ë³µì€ ìˆ˜ë™ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
