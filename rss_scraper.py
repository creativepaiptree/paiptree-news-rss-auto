#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import json
import os
import time
import hashlib
import requests
from datetime import datetime, timedelta
import sys

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
def get_config():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì •ë³´ ì½ê¸°"""
    try:
        # Google Sheets ì¸ì¦ ì •ë³´ (íŒŒì¼ì—ì„œ ì½ìŒìœ¼ë¡œ ë³€ê²½ë¨)
        with open('credentials.json') as f:
            creds_dict = json.load(f)

        # Google Sheets ID
        sheets_id = os.environ.get('GOOGLE_SHEETS_ID')
        if not sheets_id:
            raise ValueError("GOOGLE_SHEETS_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        return creds_dict, sheets_id
    except Exception as e:
        print(f"âŒ ì„¤ì • ì½ê¸° ì‹¤íŒ¨: {e}")
        sys.exit(1)

# RSS í”¼ë“œ ëª©ë¡ - ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (í‚¤ì›Œë“œë³„)
RSS_FEEDS = [
    "http://newssearch.naver.com/search.naver?where=rss&query=íŒŒì´í”„íŠ¸ë¦¬",
    "http://newssearch.naver.com/search.naver?where=rss&query=íŒŒë¨¸ìŠ¤ë§ˆì¸ë“œ", 
    "http://newssearch.naver.com/search.naver?where=rss&query=paiptree",
    "http://newssearch.naver.com/search.naver?where=rss&query=farmersmind"
]

# ê²€ìƒ‰ í‚¤ì›Œë“œ
KEYWORDS = ["íŒŒì´í”„íŠ¸ë¦¬", "íŒŒë¨¸ìŠ¤ë§ˆì¸ë“œ", "paiptree", "farmersmind"]

def setup_google_sheets(creds_dict, sheets_id):
    try:
        print("ğŸ”— Google Sheets ì—°ê²° ì¤‘...")
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        credentials = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        gc = gspread.authorize(credentials)
        sheet = gc.open_by_key(sheets_id)
        try:
            worksheet = sheet.worksheet('news_data')
            print("âœ… ê¸°ì¡´ news_data ì‹œíŠ¸ ë°œê²¬")
        except gspread.WorksheetNotFound:
            print("ğŸ“ news_data ì‹œíŠ¸ ìƒì„± ì¤‘...")
            worksheet = sheet.add_worksheet(title='news_data', rows=1000, cols=21)
            headers = [
                'id', 'title', 'description', 'category', 'tags',
                'upload_date', 'file_size', 'file_format', 'dimensions',
                'creator', 'brand_alignment', 'usage_rights', 'version',
                'download_count', 'rating', 'thumbnail_url', 'file_url',
                'original_url', 'status', 'featured', 'created_at'
            ]
            worksheet.append_row(headers)
            print("âœ… news_data ì‹œíŠ¸ ìƒì„± ì™„ë£Œ")
        return worksheet
    except Exception as e:
        print(f"âŒ Google Sheets ì—°ê²° ì‹¤íŒ¨: {e}")
        sys.exit(1)

def fetch_rss_news(rss_url, keywords):
    news_items = []
    try:
        print(f"ğŸ“¡ RSS í”¼ë“œ í™•ì¸ ì¤‘: {rss_url}")
        feed = feedparser.parse(rss_url)
        if feed.bozo:
            print(f"âš ï¸ RSS í”¼ë“œ íŒŒì‹± ê²½ê³ : {rss_url}")
        for entry in feed.entries:
            title = entry.get('title', '')
            description = entry.get('description', '') or entry.get('summary', '')
            link = entry.get('link', '')
            content_to_check = f"{title} {description}".lower()
            matched_keywords = [kw for kw in keywords if kw.lower() in content_to_check]
            if matched_keywords:
                published = entry.get('published_parsed')
                pub_date = datetime(*published[:6]).strftime('%Y-%m-%d') if published else datetime.now().strftime('%Y-%m-%d')
                source = feed.feed.get('title', 'Unknown')
                thumbnail_url = ""
                if hasattr(entry, 'media_thumbnail'):
                    thumbnail_url = entry.media_thumbnail[0]['url'] if entry.media_thumbnail else ""
                elif hasattr(entry, 'enclosures') and entry.enclosures:
                    for enc in entry.enclosures:
                        if enc.type.startswith('image/'):
                            thumbnail_url = enc.href
                            break
                news_item = {
                    'title': title[:200],
                    'description': description[:500],
                    'category': source,
                    'tags': ','.join(matched_keywords),
                    'upload_date': pub_date,
                    'download_count': 0,
                    'thumbnail_url': thumbnail_url,
                    'original_url': link
                }
                news_items.append(news_item)
                print(f"âœ… í‚¤ì›Œë“œ ë§¤ì¹­: {title[:50]}... (í‚¤ì›Œë“œ: {matched_keywords})")
        print(f"ğŸ“Š {rss_url}ì—ì„œ {len(news_items)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
        return news_items
    except Exception as e:
        print(f"âŒ RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨ {rss_url}: {e}")
        return []

def generate_sequential_id(worksheet):
    try:
        all_records = worksheet.get_all_records()
        if not all_records:
            return "001"
        max_num = 0
        for record in all_records:
            try:
                current_id = str(record.get('id', '0'))
                num = int(current_id.lstrip('0')) if current_id.strip() else 0
                max_num = max(max_num, num)
            except (ValueError, TypeError):
                continue
        return f"{max_num + 1:03d}"
    except Exception as e:
        print(f"âš ï¸ ID ìƒì„± ì‹¤íŒ¨, ì„ì‹œ ID ì‚¬ìš©: {e}")
        return f"{int(time.time() % 10000):04d}"

def is_duplicate_news(worksheet, original_url):
    try:
        all_records = worksheet.get_all_records()
        return any(record.get('original_url') == original_url for record in all_records)
    except Exception as e:
        print(f"âš ï¸ ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

def add_news_to_sheet(worksheet, news_item):
    try:
        if is_duplicate_news(worksheet, news_item['original_url']):
            print(f"â­ï¸ ì¤‘ë³µ ë‰´ìŠ¤ ìŠ¤í‚µ: {news_item['title'][:50]}...")
            return False
        news_id = generate_sequential_id(worksheet)
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        row_data = [
            news_id, news_item['title'], news_item['description'], news_item['category'], news_item['tags'],
            news_item['upload_date'], 'N/A', 'news', 'N/A', news_item['category'],
            'high', 'read-only', '1.0', news_item['download_count'], '0',
            news_item['thumbnail_url'], news_item['original_url'], news_item['original_url'], 'active', 'false', current_time
        ]
        worksheet.append_row(row_data)
        print(f"âœ… ë‰´ìŠ¤ ì¶”ê°€ (ID: {news_id}): {news_item['title'][:50]}...")
        return True
    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")
        return False

def main():
    print("ğŸš€ Paiptree ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    start_time = time.time()
    creds_dict, sheets_id = get_config()
    worksheet = setup_google_sheets(creds_dict, sheets_id)
    total_collected = 0
    total_found = 0
    print(f"ğŸ” {len(RSS_FEEDS)}ê°œ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
    print(f"ğŸ·ï¸ ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(KEYWORDS)}")
    for rss_url in RSS_FEEDS:
        news_items = fetch_rss_news(rss_url, KEYWORDS)
        total_found += len(news_items)
        for news_item in news_items:
            if add_news_to_sheet(worksheet, news_item):
                total_collected += 1
            time.sleep(0.5)
    end_time = time.time()
    execution_time = round(end_time - start_time, 2)
    print(f"\nğŸ‰ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ë°œê²¬: {total_found}ê°œ")
    print(f"âœ… ìƒˆë¡œ ì¶”ê°€: {total_collected}ê°œ")
    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time}ì´ˆ")
    if total_collected == 0:
        print("â„¹ï¸ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"ğŸŒŸ {total_collected}ê°œì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
