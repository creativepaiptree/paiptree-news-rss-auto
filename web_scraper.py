#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import time
import re
from urllib.parse import urljoin, urlparse
import logging

class ArticleImageScraper:
    def __init__(self, timeout=10, max_retries=3):
        self.timeout = timeout
        self.max_retries = max_retries
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def extract_first_image(self, article_url):
        """ê¸°ì‚¬ì—ì„œ ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ì´ë¯¸ì§€ ì¶”ì¶œ"""
        try:
            print(f"ğŸ” ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œì‘: {article_url}")
            
            # ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            response = self._fetch_with_retry(article_url)
            if not response:
                print(f"âŒ ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {article_url}")
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì´ë¯¸ì§€ í›„ë³´ë“¤ ì°¾ê¸°
            image_candidates = []
            
            # 1. og:image ë©”íƒ€ íƒœê·¸ (ê°€ì¥ ìš°ì„ )
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                image_url = og_image['content']
                if self._is_valid_image_url(image_url):
                    image_candidates.append({
                        'url': image_url,
                        'priority': 1,
                        'type': 'og_image'
                    })
                    print(f"âœ… og:image ë°œê²¬: {image_url}")
            
            # 2. article ë‚´ ì²« ë²ˆì§¸ ì´ë¯¸ì§€
            article_images = soup.find_all('img', src=True)
            for img in article_images[:10]:  # ìƒìœ„ 10ê°œë§Œ í™•ì¸
                src = img.get('src')
                if src and self._is_valid_image_url(src):
                    # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                    width = img.get('width', 0)
                    height = img.get('height', 0)
                    
                    # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ ì œì™¸ (100x100 ë¯¸ë§Œ)
                    if width and height and int(width) >= 100 and int(height) >= 100:
                        full_url = urljoin(article_url, src)
                        image_candidates.append({
                            'url': full_url,
                            'priority': 2,
                            'type': 'article_image',
                            'width': width,
                            'height': height
                        })
                        print(f"âœ… article ì´ë¯¸ì§€ ë°œê²¬: {full_url} ({width}x{height})")
                        break  # ì²« ë²ˆì§¸ ìœ íš¨í•œ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©
            
            # 3. ê¸°íƒ€ ì´ë¯¸ì§€ (fallback)
            if not image_candidates:
                for img in soup.find_all('img', src=True):
                    src = img.get('src')
                    if src and self._is_valid_image_url(src):
                        full_url = urljoin(article_url, src)
                        image_candidates.append({
                            'url': full_url,
                            'priority': 3,
                            'type': 'fallback'
                        })
                        print(f"âœ… fallback ì´ë¯¸ì§€ ë°œê²¬: {full_url}")
                        break
            
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë ¬
            image_candidates.sort(key=lambda x: x['priority'])
            
            if image_candidates:
                selected_image = image_candidates[0]
                print(f"ğŸ¯ ì„ íƒëœ ì´ë¯¸ì§€: {selected_image['url']} (íƒ€ì…: {selected_image['type']})")
                return selected_image['url']
            else:
                print(f"âŒ ìœ íš¨í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {article_url}")
                return None
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨ {article_url}: {e}")
            return None
    
    def _fetch_with_retry(self, url):
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(self.max_retries):
            try:
                print(f"ğŸ“¡ ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹œë„ {attempt + 1}/{self.max_retries}: {url}")
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                print(f"âœ… ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {url}")
                return response
            except Exception as e:
                print(f"âš ï¸ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                if attempt < self.max_retries - 1:
                    sleep_time = 2 ** attempt
                    print(f"â³ {sleep_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    time.sleep(sleep_time)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    print(f"ğŸ’¥ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨: {url}")
                    return None
    
    def _is_valid_image_url(self, url):
        """ìœ íš¨í•œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸"""
        if not url:
            return False
        
        # ìƒëŒ€ URLì€ ìœ íš¨í•˜ë‹¤ê³  ê°€ì •
        if url.startswith('/'):
            return True
        
        # ì ˆëŒ€ URL ê²€ì¦
        try:
            parsed = urlparse(url)
            if parsed.scheme not in ['http', 'https']:
                return False
            
            # ì´ë¯¸ì§€ íŒŒì¼ í™•ì¥ì í™•ì¸
            image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']
            path_lower = parsed.path.lower()
            if any(path_lower.endswith(ext) for ext in image_extensions):
                return True
            
            # í™•ì¥ìê°€ ì—†ì–´ë„ ìœ íš¨í•  ìˆ˜ ìˆìŒ (ë™ì  ì´ë¯¸ì§€)
            return True
            
        except Exception:
            return False 